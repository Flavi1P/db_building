{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the PAR ncdfs into one xarray data object, with date as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where your NetCDF files are stored (daily files)\n",
    "ncdf_dir = \"C:/Users/flapet/OneDrive - NOC/Documents/IDAPro/lib/db_building/data/satellite/par_mapped/\"\n",
    "\n",
    "# List all NetCDF files for the year (one file per day)\n",
    "ncdf_files = sorted([f for f in os.listdir(ncdf_dir) if f.endswith('.nc')])\n",
    "\n",
    "# Load the PAR data for each file and store it\n",
    "daily_par_data = {}\n",
    "for file in ncdf_files:\n",
    "    # Open the dataset for the specific day\n",
    "    ds = xr.open_dataset(os.path.join(ncdf_dir, file))\n",
    "    \n",
    "    # Extract PAR data (assuming it's named 'par', adjust if necessary)\n",
    "    # Add a date key to use as the dictionary key for each day\n",
    "    date = file.split('.')[1] # Extract the date from the filename (adjust based on filename format)\n",
    "    \n",
    "    # Store the data in a dictionary, with the date as the key\n",
    "    daily_par_data[date] = ds[\"par\"]  # Replace 'par' with the actual variable name in your files\n",
    "\n",
    "# Example: inspect one of the datasets\n",
    "print(daily_par_data[\"20240101\"])  # Inspect the PAR data for January 1st, 2024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open BGC Argo parquet and summarise to one line per profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#argo = pq.read_table('C:/Users/flapet/OneDrive - NOC/Documents/IDAPro/lib/db_building/data/argo_pq/biocarbon_floats_table.parquet')\n",
    "argo = pq.read_table('C:/Users/flapet/OneDrive - NOC/Documents/IDAPro/lib/db_building/data/argo_pq/biocarbon_floats_table.parquet')\n",
    "obs_data = argo.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_data = obs_data[obs_data['JULD'] < pd.to_datetime(\"2025-01-01\")]\n",
    "obs_data = obs_data[obs_data['PRES'] < 201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mad_despike(series, threshold=3.5):\n",
    "    \"\"\" Remove spikes using MAD method \"\"\"\n",
    "    median = np.nanmedian(series)\n",
    "    mad = np.nanmedian(np.abs(series - median))\n",
    "    modified_z_score = 0.6745 * (series - median) / mad if mad else np.zeros_like(series)\n",
    "    return series[np.abs(modified_z_score) < threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def despike_group(group):\n",
    "    group[\"bbp700_cleaned\"] = mad_despike(group[\"BBP700_ADJUSTED\"])\n",
    "    return group\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_data = obs_data.groupby([\"JULD\"], group_keys=False).apply(despike_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "def get_nearest_par(lat, lon, date, daily_par_data):\n",
    "    \"\"\"Find the nearest PAR value for a given lat, lon, and date.\"\"\"\n",
    "    if date in daily_par_data and daily_par_data[date].size > 0:\n",
    "        par_data = daily_par_data[date]\n",
    "        nearest_par = par_data.sel(lon=lon, lat=lat, method='nearest').values.flatten()[0]\n",
    "    else:\n",
    "        nearest_par = 0\n",
    "    return nearest_par\n",
    "\n",
    "# Step 1: Extract unique position-time combinations\n",
    "unique_obs = obs_data[[\"LATITUDE\", \"LONGITUDE\", \"JULD\"]].drop_duplicates()\n",
    "\n",
    "# Step 2: Compute PAR only for unique combinations\n",
    "unique_obs[\"satellite_par\"] = unique_obs.apply(\n",
    "    lambda row: get_nearest_par(\n",
    "        row[\"LATITUDE\"], row[\"LONGITUDE\"], row[\"JULD\"].strftime('%Y%m%d'),\n",
    "        daily_par_data\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Step 3: Merge results back to the full dataset\n",
    "obs_data = obs_data.merge(unique_obs, on=[\"LATITUDE\", \"LONGITUDE\", \"JULD\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularize data every meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Round the `PRES` column to the nearest integer\n",
    "obs_data['PRES_rounded'] = round(obs_data['PRES'], 0)\n",
    "\n",
    "# Step 2: Group by the rounded `PRES` values\n",
    "# Aggregate other columns by taking the mean for each group\n",
    "df_regularized = obs_data\n",
    "\n",
    "# Step 3: Drop the original `PRES` column if not needed\n",
    "df_regularized = df_regularized.drop(columns=['PRES'])\n",
    "\n",
    "df_regularized['year'] = df_regularized['JULD'].dt.year\n",
    "df_regularized['month'] = df_regularized['JULD'].dt.month\n",
    "df_regularized['day'] = df_regularized['JULD'].dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the primary production algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regularized['BBP470'] = df_regularized['bbp700_cleaned']/(470/400) \n",
    "df_regularized['carbon'] = 12128 * df_regularized['BBP470'] + 0.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_depth = pd.merge(\n",
    "    df_regularized[[\"JULD\"]].drop_duplicates().assign(key=1),\n",
    "    df_regularized[[\"PRES_rounded\"]].drop_duplicates().assign(key=1),\n",
    "    on=\"key\"\n",
    ").drop(columns=\"key\")\n",
    "\n",
    "# Step 2: Filter where PRES_ROUNDED < 201\n",
    "full_depth = full_depth[full_depth[\"PRES_rounded\"] < 201]\n",
    "\n",
    "# Step 3: Left join with the original dataframe\n",
    "full_depth = full_depth.merge(df_regularized, on=[\"JULD\", \"PRES_rounded\"], how=\"left\")\n",
    "\n",
    "# Step 4: Group by (JULD, PRES_ROUNDED) and compute mean, ignoring NaNs\n",
    "full_depth = (\n",
    "    full_depth\n",
    "    .groupby([\"JULD\", \"PRES_rounded\"], as_index=False)\n",
    "    .agg(lambda x: np.nanmean(x) if np.issubdtype(x.dtype, np.number) else x.iloc[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opp_befa(chl, irr, sst, dayL):\n",
    "    if chl < 1.0:\n",
    "        chl_tot = 38.0 * np.power(chl, 0.425)\n",
    "    else:\n",
    "        chl_tot = 40.2 * np.power(chl, 0.507)\n",
    "\n",
    "    z_eu = 200.0 * np.power(chl_tot, (-0.293))\n",
    "\n",
    "    if z_eu <= 102.0:\n",
    "         z_eu = 568.2 * np.power(chl_tot, -0.746)\n",
    "\n",
    "    if sst < -10.0:\n",
    "        pb_opt = 0.0\n",
    "    elif sst < -1.0:\n",
    "        pb_opt = 1.13\n",
    "    elif sst > 28.5:\n",
    "        pb_opt = 4.0\n",
    "    else:\n",
    "        pb_opt = 1.2956 + 2.749e-1*sst + 6.17e-2*np.power(sst, 2) - \\\n",
    "            2.05e-2*np.power(sst, 3) + 2.462e-3*np.power(sst, 4) - \\\n",
    "            1.348e-4*np.power(sst, 5) + 3.4132e-6*np.power(sst, 6) - \\\n",
    "            3.27e-8*np.power(sst, 7)\n",
    "\n",
    "    irrFunc = 0.66125 * irr / (irr + 4.1)\n",
    "\n",
    "    npp = pb_opt * chl * dayL * irrFunc * z_eu\n",
    "\n",
    "    return npp\n",
    "\n",
    "def cal_dayL(lat, yDay):\n",
    "    gamma = lat/180.0 * np.pi\n",
    "    psi = yDay/365.0 * 2.0 * np.pi\n",
    "    solarDec = (0.39637 - 22.9133*np.cos(psi) + 4.02543*np.sin(psi) - \\\n",
    "                0.38720*np.cos(2*psi) + 0.05200*np.sin(2*psi)) * np.pi/180.0\n",
    "    r = -np.tan(gamma) * np.tan(solarDec)\n",
    "\n",
    "    if r<=-1:\n",
    "        return 24.0\n",
    "    elif np.fabs(r)<1:\n",
    "        return 24.0 * np.arccos(r) / np.pi\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def day_of_year(day, month, year=2024):\n",
    "    return (datetime.date(year, month, day) - datetime.date(year, 1, 1)).days + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(array, window_size=5):\n",
    "    kernel = np.ones(window_size) / window_size\n",
    "    smoothed_array = np.convolve(array, kernel, mode='same')  # 'same' ensures the output matches input size\n",
    "    return smoothed_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbpm_argo import cbpm_argo\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import datetime\n",
    "\n",
    "dfs = []\n",
    "depth_grid = np.arange(0,200)\n",
    "\n",
    "# Iterate through each unique 'JULD' (day)\n",
    "for i in full_depth['JULD'].unique():\n",
    "    # Filter for rows corresponding to the current 'JULD'\n",
    "    temp_df = full_depth[full_depth['JULD'] == i].iloc[1:201,].copy()  # Use `.copy()` to avoid warnings\n",
    "\n",
    "    # Extract the pressure and chlorophyll values for interpolation\n",
    "    pres_values = temp_df['PRES_rounded'].to_numpy()\n",
    "    chl_values = temp_df['CHLA_ADJUSTED'].to_numpy()\n",
    "    carbon_values = temp_df['carbon'].to_numpy()\n",
    "    temp_values = temp_df['TEMP'].to_numpy()\n",
    "\n",
    "    # Apply the running mean smoothing\n",
    "    chl_smoothed = running_mean(chl_values, window_size=5)\n",
    "    carbon_smoothed = running_mean(carbon_values, window_size=5)\n",
    "\n",
    "    # Check for valid data before interpolation (avoid NaN values)\n",
    "    mask = ~np.isnan(pres_values) & ~np.isnan(chl_values)\n",
    "    pres_values = pres_values[mask]\n",
    "    chl_values = chl_values[mask]\n",
    "    carbon_values = carbon_values[mask]\n",
    "    temp_values = temp_values[mask]\n",
    "\n",
    "    # Interpolate the CHLA_ADJUSTED onto the depth grid (0 to 199)\n",
    "    if len(pres_values) > 1:  # Ensure there's enough data to interpolate\n",
    "        interpolator = interp1d(pres_values, chl_values, bounds_error=False, fill_value=np.nan)\n",
    "        interpolated_chl = interpolator(depth_grid)\n",
    "        \n",
    "        interpolator = interp1d(pres_values, carbon_values, bounds_error=False, fill_value=np.nan)\n",
    "        interpolated_carbon = interpolator(depth_grid)\n",
    "\n",
    "        interpolator = interp1d(pres_values, temp_values, bounds_error=False, fill_value=np.nan)\n",
    "        interpolated_temp = interpolator(depth_grid)\n",
    "        \n",
    "    else:\n",
    "        # If only one point or no valid data, fill with NaN\n",
    "        print(temp_df['JULD'].unique())\n",
    "        interpolated_chl = np.full(depth_grid.shape, np.nan)\n",
    "        interpolated_carbon = np.full(depth_grid.shape, np.nan)\n",
    "\n",
    "    # Now we can extract other values and apply the cbpm_argo function\n",
    "    chl_z = interpolated_chl\n",
    "    Cphyto_z = interpolated_carbon\n",
    "    irr = temp_df['satellite_par'].mean()  # Mean irradiance value\n",
    "    year = int(temp_df['year'].mean())\n",
    "    month = int(temp_df['month'].mean())\n",
    "    day = int(temp_df['day'].mean())\n",
    "    lat = temp_df['LATITUDE'].mean()\n",
    "    sst = interpolated_temp[0:5].mean()\n",
    "\n",
    "    #calculation of daylength\n",
    "    doy = day_of_year(day, month)\n",
    "    day_length = cal_dayL(lat, doy)\n",
    "\n",
    "    # Call the cbpm_argo function with interpolated data\n",
    "    [pp_z, mu_z, par_z, prcnt_z, nutTempFunc_z, IgFunc_z, mzeu] = cbpm_argo(chl_z, Cphyto_z, irr, year, month, day, lat)\n",
    "\n",
    "    #VGPM computation\n",
    "    npp_vgpm = opp_befa(chl_z[0], irr, sst, day_length)\n",
    "\n",
    "    size_max = len(temp_df)\n",
    "\n",
    "    # Use .loc to explicitly assign new columns (expand results back into DataFrame)\n",
    "    temp_df.loc[:, 'pp'] = pp_z[0:size_max]\n",
    "    temp_df.loc[:, 'mu'] = mu_z[0:size_max]\n",
    "    temp_df.loc[:, 'prcnt'] = prcnt_z[0:size_max]\n",
    "    temp_df.loc[:, 'nutTempFunc'] = nutTempFunc_z[0:size_max]\n",
    "    temp_df.loc[:, 'IgFunc'] = IgFunc_z[0:size_max]\n",
    "    temp_df.loc[:, 'zeu'] = np.full(size_max, mzeu)\n",
    "    temp_df.loc[:, 'npp_vgpm'] = np.full(size_max, npp_vgpm)\n",
    "\n",
    "    # Append modified DataFrame to the list\n",
    "    dfs.append(temp_df)\n",
    "\n",
    "\n",
    "# Combine all DataFrames\n",
    "final_df = pd.concat(dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('C:/Users/flapet/OneDrive - NOC/Documents/IDAPro/lib/db_building/data/argo_pp_estimations_floats_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_regularized[df_regularized[\"PLATFORM_NUMBER\"] == \"4903532 \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('C:/Users/flapet/OneDrive - NOC/Documents/IDAPro/lib/db_building/data/problematic_float.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
