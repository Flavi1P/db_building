{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b097cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from haversine import haversine_vector, Unit\n",
    "import glob\n",
    "import xarray as xr\n",
    "import polars as pl\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f89082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of NetCDF files\n",
    "nc_files = glob.glob(\"C:/Users/flapet/OneDrive - NOC/Documents/IDAPro/lib/db_building/data/glider/nc_profiled/*.nc\")\n",
    "\n",
    "# Read and concatenate datasets\n",
    "for file in nc_files:\n",
    "    ds = xr.open_dataset(file, decode_times=True)\n",
    "    glider_name = ds.attrs.get(\"trajectory\").split(\"_\")[0]\n",
    "    # Select variables of interest\n",
    "    vars_of_interest = [\"TIME\", \"DEPTH\", \"TEMP\", \"CHLA\", \"BBP700\", \"ABS_SALINITY\", \"MOLAR_DOXY\", \"PROFILE_NUMBER\", \"LATITUDE\", \"LONGITUDE\"]\n",
    "    ds_sel = ds[vars_of_interest]\n",
    "    # Convert to pandas DataFrame, then to polars DataFrame\n",
    "    df_pd = ds_sel.to_dataframe().reset_index()\n",
    "    df_pl = pl.from_pandas(df_pd)\n",
    "\n",
    "    #Use only profiling data (-1 is surfacing behavior)\n",
    "    df_pl = df_pl.filter(pl.col(\"PROFILE_NUMBER\") > 0)\n",
    "\n",
    "    df_pl = df_pl.with_columns((pl.col(\"PROFILE_NUMBER\").cast(pl.String) + \"_\" + glider_name).alias(\"profile_id\"))\n",
    "    print(f\"{file} processed, shape: {df_pl.shape}\")\n",
    "\n",
    "    # Concatenate DataFrames\n",
    "    if 'df_concat' in locals():\n",
    "        df_concat = pl.concat([df_concat, df_pl])\n",
    "    else:\n",
    "        df_concat = df_pl\n",
    "\n",
    "# Add a 'glider_name' column by extracting the part before '_' in 'profile_id'\n",
    "df_concat = df_concat.with_columns(\n",
    "    pl.col(\"profile_id\").str.split(\"_\").list.get(1).alias(\"glider_name\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f48dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = (\n",
    "    df_concat\n",
    "    .sort(['profile_id', 'TIME'])\n",
    "    .with_columns([\n",
    "        pl.col('DEPTH')\n",
    "        .interpolate()\n",
    "        .over('profile_id')\n",
    "        .alias('DEPTH')\n",
    "    ])\n",
    ")\n",
    "df = df_concat.to_pandas().copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.dropna(subset=[\"DEPTH\", \"LONGITUDE\", \"LATITUDE\", \"TIME\"])\n",
    "# Split by glider\n",
    "df_cabot = df[df['profile_id'].str.contains(\"Cabot\")]\n",
    "df_churchill = df[df['profile_id'].str.contains(\"Churchill\")]\n",
    "\n",
    "df_cabot['depth_bin'] = (df_cabot['DEPTH'] // 2 * 2).astype(int)\n",
    "df_churchill['depth_bin'] = (df_churchill['DEPTH'] // 2 * 2).astype(int)\n",
    "\n",
    "df_cabot_binned = df_cabot.groupby(['profile_id', 'depth_bin'])[[\"TEMP\", \"ABS_SALINITY\", \"CHLA\", \"MOLAR_DOXY\", \"BBP700\"]].median().reset_index()\n",
    "df_cabot_binned = df_cabot_binned.dropna(subset=[\"TEMP\", \"ABS_SALINITY\", \"CHLA\", \"MOLAR_DOXY\", \"BBP700\"])\n",
    "df_churchill_binned = df_churchill.groupby(['profile_id', 'depth_bin'])[[\"TEMP\", \"ABS_SALINITY\", \"CHLA\", \"MOLAR_DOXY\", \"BBP700\"]].median().reset_index()\n",
    "df_churchill_binned = df_churchill_binned.dropna(subset=[\"TEMP\", \"ABS_SALINITY\", \"CHLA\", \"MOLAR_DOXY\", \"BBP700\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc78ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_metadata(df):\n",
    "    meta = df.groupby(['profile_id']).agg({\n",
    "        'LATITUDE': 'first',\n",
    "        'LONGITUDE': 'first',\n",
    "        'TIME': 'first'\n",
    "    }).reset_index()\n",
    "    meta['time_days'] = (meta['TIME'] - meta['TIME'].min()).dt.total_seconds() / 86400\n",
    "    return meta\n",
    "\n",
    "meta = profile_metadata(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_cabot = meta[meta['profile_id'].str.contains(\"Cabot\")]\n",
    "meta_churchill = meta[meta['profile_id'].str.contains(\"Churchill\")]\n",
    "\n",
    "# Convert time to days since start\n",
    "t0 = min(df['TIME'])\n",
    "df_cabot['time_days'] = (df_cabot['TIME'] - t0).dt.total_seconds() / 86400\n",
    "df_churchill['time_days'] = (df_churchill['TIME'] - t0).dt.total_seconds() / 86400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455dd213",
   "metadata": {},
   "source": [
    "KDtree matching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c999405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_profiles(meta_A, meta_B, alpha=10, max_dist_km=50):\n",
    "    \"\"\"\n",
    "    Match profiles between meta_A and meta_B using a combined space-time distance.\n",
    "    \n",
    "    Parameters:\n",
    "    - alpha: km/day scaling factor for time\n",
    "    - max_dist_km: maximum combined distance in km\n",
    "    \n",
    "    Returns:\n",
    "    - Filtered matched profiles from A and B\n",
    "    \"\"\"\n",
    "    # Build scaled 3D coordinates: lat, lon, and scaled time\n",
    "    coords_A = np.column_stack([\n",
    "        meta_A['LATITUDE'],\n",
    "        meta_A['LONGITUDE'],\n",
    "        meta_A['time_days'] * alpha  # time scaled to km\n",
    "    ])\n",
    "    coords_B = np.column_stack([\n",
    "        meta_B['LATITUDE'],\n",
    "        meta_B['LONGITUDE'],\n",
    "        meta_B['time_days'] * alpha\n",
    "    ])\n",
    "    \n",
    "    tree_B = cKDTree(coords_B)\n",
    "    dist, idx = tree_B.query(coords_A, k=1)  # combined distance\n",
    "\n",
    "    # Retrieve matched rows\n",
    "    matched_B = meta_B.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "    # Filter by combined spatio-temporal distance\n",
    "    mask = dist <= max_dist_km\n",
    "\n",
    "    return meta_A[mask].reset_index(drop=True), matched_B[mask].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37aa06",
   "metadata": {},
   "source": [
    "Profile comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b53aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_profiles(df_A, df_B, meta_A, meta_B, var):\n",
    "    rmse_list = []\n",
    "    r2_list = []\n",
    "\n",
    "    var_a = var + \"_a\"\n",
    "    var_b = var + \"_b\"\n",
    "    for i in range(len(meta_A)):\n",
    "        pa = df_A[df_A['profile_id'] == meta_A.iloc[i]['profile_id']]\n",
    "        pb = df_B[df_B['profile_id'] == meta_B.iloc[i]['profile_id']]\n",
    "\n",
    "        merged = pd.merge(pa[['depth_bin', var]], pb[['depth_bin', var]], on='depth_bin', suffixes=('_a', '_b'))\n",
    "        if len(merged) > 10:\n",
    "            rmse = root_mean_squared_error(merged[var_a], merged[var_b])\n",
    "            rmse_list.append(rmse)\n",
    "\n",
    "            r2 = r2_score(merged[var_a], merged[var_b])\n",
    "            r2_list.append(r2)\n",
    "        else:\n",
    "            rmse_list.append(np.nan)\n",
    "            r2_list.append(np.nan)\n",
    "    \n",
    "    # Convert to arrays\n",
    "    r2_array = np.array(r2_list)\n",
    "\n",
    "    # Compute median R² over valid (non-NaN) entries\n",
    "    valid_r2 = r2_array[~np.isnan(r2_array)]\n",
    "    median_r2 = np.median(valid_r2) if len(valid_r2) > 0 else np.nan\n",
    "\n",
    "\n",
    "    return np.array(rmse_list), np.array(median_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52aa293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_profiles2(df_A, df_B, meta_A, meta_B, var):\n",
    "    temps_a_all = []\n",
    "    temps_b_all = []\n",
    "    depth_all = []\n",
    "\n",
    "    var_a = var + '_a'\n",
    "    var_b = var + '_b'\n",
    "\n",
    "    for i in range(len(meta_A)):\n",
    "        pid_a = meta_A.iloc[i]['profile_id']\n",
    "        pid_b = meta_B.iloc[i]['profile_id']\n",
    "\n",
    "        prof_a = df_A[df_A['profile_id'] == pid_a]\n",
    "        prof_b = df_B[df_B['profile_id'] == pid_b]\n",
    "\n",
    "        merged = pd.merge(\n",
    "            prof_a[['depth_bin', var]],\n",
    "            prof_b[['depth_bin', var]],\n",
    "            on='depth_bin',\n",
    "            suffixes=('_a', '_b')\n",
    "        )\n",
    "\n",
    "        if len(merged) >= 10:\n",
    "            temps_a_all.extend(merged[var_a].values)\n",
    "            temps_b_all.extend(merged[var_b].values)\n",
    "            depth_all.extend(merged['depth_bin'].values)\n",
    "\n",
    "    if len(temps_a_all) == 0:\n",
    "        print(\"No valid profile matches with sufficient depth overlap.\")\n",
    "        return\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    x = np.array(temps_a_all)\n",
    "    y = np.array(temps_b_all)\n",
    "    z = np.array(depth_all)\n",
    "\n",
    "    # Compute stats\n",
    "    rmse = root_mean_squared_error(x, y)\n",
    "    r2 = r2_score(x, y)\n",
    "    return rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1940642d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "best_params = None\n",
    "\n",
    "time_scales = np.linspace(5, 30, 5)        # km/day\n",
    "\n",
    "for t_scale in time_scales:\n",
    "    matched_A, matched_B = match_profiles(meta_cabot.copy(), meta_churchill.copy(), t_scale, max_days=1, max_km=30)\n",
    "    print(f\"{len(matched_A)} profile pairs matched within 2 days and 50 km.\")\n",
    "    avg_rmse, avg_r2 = compare_profiles2(df_cabot_binned, df_churchill_binned, matched_A, matched_B, 'TEMP')\n",
    "    if avg_r2 > best_score:\n",
    "        best_score = avg_r2\n",
    "        best_rmse = avg_rmse\n",
    "        best_params = t_scale\n",
    "\n",
    "print(f\"Best scale: time = {best_params:.2f} km/day, R2 = {best_score:.2f}\")\n",
    "print(f\"Best RMSE = {best_rmse:.3f}, mean R² = {best_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf01b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_matched_scatter(df_A, df_B, meta_A, meta_B):\n",
    "    temps_a_all = []\n",
    "    temps_b_all = []\n",
    "    depth_all = []\n",
    "\n",
    "    for i in range(len(meta_A)):\n",
    "        pid_a = meta_A.iloc[i]['profile_id']\n",
    "        pid_b = meta_B.iloc[i]['profile_id']\n",
    "\n",
    "        prof_a = df_A[df_A['profile_id'] == pid_a]\n",
    "        prof_b = df_B[df_B['profile_id'] == pid_b]\n",
    "\n",
    "        merged = pd.merge(\n",
    "            prof_a[['depth_bin', 'CHLA']],\n",
    "            prof_b[['depth_bin', 'CHLA']],\n",
    "            on='depth_bin',\n",
    "            suffixes=('_a', '_b')\n",
    "        )\n",
    "\n",
    "        if len(merged) >= 10:\n",
    "            temps_a_all.extend(merged['CHLA_a'].values)\n",
    "            temps_b_all.extend(merged['CHLA_b'].values)\n",
    "            depth_all.extend(merged['depth_bin'].values)\n",
    "\n",
    "    if len(temps_a_all) == 0:\n",
    "        print(\"No valid profile matches with sufficient depth overlap.\")\n",
    "        return\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    x = np.array(temps_a_all)\n",
    "    y = np.array(temps_b_all)\n",
    "    z = np.array(depth_all)\n",
    "\n",
    "    # Compute stats\n",
    "    rmse = root_mean_squared_error(x, y)\n",
    "    r2 = r2_score(x, y)\n",
    "    r, _ = pearsonr(x, y)\n",
    "\n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    sc = plt.scatter(x, y, c=z, cmap='viridis', s=30, alpha=0.7, edgecolor='k', linewidth=0.3)\n",
    "    plt.plot([min(x), max(x)], [min(x), max(x)], 'r--', label='1:1 line')\n",
    "    plt.xlabel('Temperature (°C) - Glider A')\n",
    "    plt.ylabel('Temperature (°C) - Glider B')\n",
    "    plt.title(f'All Matched Profile Points\\n(RMSE={rmse:.2f}, R²={r2:.2f}, r={r:.2f})')\n",
    "\n",
    "    cbar = plt.colorbar(sc)\n",
    "    cbar.set_label('Depth (m)')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623d2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_A, matched_B = match_profiles(meta_cabot.copy(), meta_churchill.copy(), 17.5, max_days=1, max_km=30)\n",
    "plot_all_matched_scatter(df_cabot_binned, df_churchill_binned, matched_A, matched_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e1b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define grid\n",
    "time_scales = np.linspace(1, 50, 5)        # km/day\n",
    "varlist = [\"TEMP\", \"ABS_SALINITY\", \"CHLA\", \"MOLAR_DOXY\", \"BBP700\"]   # km/m\n",
    "\n",
    "# Store R² values\n",
    "r2_matrix = np.full((len(varlist), len(time_scales)), np.nan)\n",
    "\n",
    "# Fill matrix with mean R²\n",
    "for i, t_scale in enumerate(time_scales):\n",
    "    for j, var in enumerate(varlist):\n",
    "        matched_A, matched_B = match_profiles(meta_cabot.copy(), meta_churchill.copy(), t_scale, max_days=1, max_km=50)\n",
    "        rmse_vals, avg_r2 = compare_profiles2(df_cabot_binned, df_churchill_binned, matched_A, matched_B, var)\n",
    "        r2_matrix[j, i] = avg_r2\n",
    "        print(f\"Time scale: {t_scale:.2f} km/day, Variable: {var}, Mean R²: {r2_matrix[j, i]:.2f}\")\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Replace NaN values with a placeholder (e.g., 0) or use interpolation\n",
    "r2_matrix_cleaned = np.nan_to_num(r2_matrix, nan=0)\n",
    "\n",
    "sns.heatmap(r2_matrix_cleaned, xticklabels=[f\"{s:.0f}\" for s in time_scales],\n",
    "            yticklabels=varlist,\n",
    "            cmap=\"PuBu\", annot=True, fmt=\".2f\", cbar_kws={'label': 'Mean R²'})\n",
    "\n",
    "plt.title(\"Mean R² for Profile Matching vs. Time and Depth Scale\")\n",
    "plt.xlabel(\"Time Scale (km/day)\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dff7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid\n",
    "max_km_list = np.linspace(100, 1000, 5)        # km/day\n",
    "varlist = [\"TEMP\", \"ABS_SALINITY\", \"CHLA\", \"MOLAR_DOXY\"]   # km/m\n",
    "\n",
    "# Store R² values\n",
    "r2_matrix = np.full((len(max_days_list), len(varlist)), np.nan)\n",
    "# Fill matrix with mean R²\n",
    "for i, max_km in enumerate(max_km_list):\n",
    "    for j, var in enumerate(varlist):\n",
    "        matched_A, matched_B = match_profiles(meta_cabot.copy(), meta_churchill.copy(), 20, max_dist_km=max_km)\n",
    "        rmse_vals, avg_r2 = compare_profiles(df_cabot_binned, df_churchill_binned, matched_A, matched_B, var)\n",
    "        r2_matrix[i, j] = avg_r2\n",
    "        print(f\"Max dist: {max_km:.2f} km, Variable: {var}, Mean R²: {r2_matrix[i, j]:.2f}\")\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Replace NaN values with a placeholder (e.g., 0) or use interpolation\n",
    "r2_matrix_cleaned = np.nan_to_num(r2_matrix, nan=0)\n",
    "\n",
    "sns.heatmap(r2_matrix_cleaned, xticklabels=[f\"{s:.0f}\" for s in max_km_list],\n",
    "            yticklabels=varlist,\n",
    "            cmap=\"PuBu\", annot=True, fmt=\".2f\", cbar_kws={'label': 'Mean R²'})\n",
    "\n",
    "plt.title(\"Mean R² for Profile Matching vs. Time and Depth Scale\")\n",
    "plt.xlabel(\"Time Scale (km/day)\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
