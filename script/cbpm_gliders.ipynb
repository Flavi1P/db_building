{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from cmocean import cm as cmo\n",
    "import glidertools as gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = xr.open_dataset(\"C:/Users/flapet/OneDrive - NOC/Documents/IDAPro/lib/db_building/data/glider/pomBODCREQ-5915/unit_345/nc_files/L0-timeseries/Biocarbon_cabot.nc\", chunks={\"time\": 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (dat.depth < 400).compute()\n",
    "dat_surf = dat.where(mask, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.plot(dat_surf['profile_index'], dat_surf['depth'], dat_surf['backscatter_700'], cmap=cmo.delta)\n",
    "plt.title('b$_{bp}$ (m$^{-1}$)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbp_1d = np.array(dat_surf['backscatter_700'])\n",
    "prof_1d = np.array(dat_surf['profile_index'])\n",
    "depth_1d = np.array(dat_surf['pressure'])\n",
    "\n",
    "nan_mask = np.isnan(depth_1d)\n",
    "valid_idx = np.where(~nan_mask)[0]  # Indices of non-NaN values\n",
    "valid_values = depth_1d[valid_idx]   # Non-NaN values\n",
    "depth_1d[nan_mask] = np.interp(np.where(nan_mask)[0], valid_idx, valid_values)\n",
    "\n",
    "\n",
    "#bbp_horz = gt.cleaning.horizontal_diff_outliers(prof_1d, depth_1d, bbp_1d, depth_threshold=10, mask_frac=0.05)\n",
    "bbp_baseline, bbp_spikes = gt.cleaning.despike(bbp_1d, 7, spike_method='minmax')\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(3, 1, figsize=[9, 6], sharex=True, dpi=90)\n",
    "\n",
    "# gt.plot(prof_1d, depth_1d, bbp_1d, cmap=cmo.delta, ax=ax[0], robust=True)\n",
    "# gt.plot(prof_1d, depth_1d, bbp_baseline, cmap=cmo.delta, ax=ax[1], robust=True)\n",
    "# gt.plot(prof_1d, depth_1d, bbp_spikes, ax=ax[2], cmap=plt.cm.Spectral_r, vmin=0, vmax=0.004)\n",
    "\n",
    "# [a.set_xlabel('') for a in ax]\n",
    "\n",
    "# ax[0].set_title('Raw b$_{bp}$ (m$^{-1}$)')\n",
    "# ax[1].set_title('Despiked b$_{bp}$ (m$^{-1}$)')\n",
    "# ax[2].set_title('b$_{bp}$ (m$^{-1}$) spikes')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_surf['bbp700'] = (\"time\", bbp_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = (dat_surf.profile_index > 0).compute()\n",
    "# prof_i = dat_surf.where(mask, drop = True)\n",
    "\n",
    "# bbp_raw = prof_i['backscatter_700']\n",
    "# bbp_despiked = prof_i['bbp700']\n",
    "# depths = prof_i['depth'].compute()\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# #plt.plot(bbp_raw, depths, label='Backscatter Raw', color='blue')\n",
    "# plt.scatter(bbp_despiked, depths, label='BBP Despiked', color='orange')\n",
    "\n",
    "# plt.gca().invert_yaxis()  # Depth increases downwards\n",
    "# plt.xlabel(\"Backscatter / BBP\")\n",
    "# plt.ylabel(\"Depth (m)\")\n",
    "# plt.xlim(0,0.01)\n",
    "# plt.legend()\n",
    "# plt.title(\"Profile 352: Despiking result\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt.plot(dat_surf['profile_index'], dat_surf['depth'], dat_surf['chlorophyll'], cmap=cmo.delta)\n",
    "# plt.title(\"Chlorophyll\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flr_iqr = gt.cleaning.outlier_bounds_iqr(dat_surf['chlorophyll'], multiplier=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flr_dark = gt.optics.fluorescence_dark_count(flr_iqr, dat_surf.depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.plot(dat_surf['profile_index'], dat_surf['depth'], flr_dark, cmap=cmo.delta, robust=True)\n",
    "plt.title('dark corrected fluorescence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flr_base, flr_spikes = gt.cleaning.despike(flr_dark, 11, spike_method='median')\n",
    "\n",
    "# fig, ax = plt.subplots(2, 1, figsize=[9, 6], sharex=True, dpi=90)\n",
    "\n",
    "# gt.plot(dat_surf['profile_index'], dat_surf['depth'], flr_base, cmap=cmo.delta, ax=ax[0], robust=True)\n",
    "# gt.plot(dat_surf['profile_index'], dat_surf['depth'], flr_spikes, cmap=plt.cm.RdBu_r, ax=ax[1], vmin=0, vmax=4)\n",
    "\n",
    "# [a.set_xlabel('') for a in ax]\n",
    "# [a.set_ylim(300, 0) for a in ax]\n",
    "\n",
    "# ax[0].set_title('Despiked Fluorescence')\n",
    "# ax[1].set_title('Fluorescence spikes')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_surf = dat_surf.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quenching correction\n",
    "\n",
    "In the following cell we will perform quenching correction. \n",
    "\n",
    "The first step is to define night profiles, we selected every profiles between 10pm and 4am UTC, to be sure to avoid quenching. (Thus this can't be applied straight to other dataset where this time period might be quenched).\n",
    "Then we define a fluo/bbp ratio on the first 40m of the profile, averaged.\n",
    "\n",
    "On the day profiles, we identify the depth of the fluorescence maximum as the layer of potential quenching. We then compare the fluo/bbp ratio of the day profile to the night one. I it is lower that 95% of the night value (i.e. 5% difference) we recompute a corrected fluorescence as being bbp700 * fluo/bbp700_at_night\n",
    "\n",
    "Everything is then appended back to the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert our fluo bbp dataset to a polar dataframe because it is easier.\n",
    "data = [\n",
    "    pl.Series(\"datetime\",  dat_surf.time.data, dtype=pl.Datetime),\n",
    "    pl.Series(\"prof\", dat_surf.profile_index.data, dtype=pl.Float64),\n",
    "    pl.Series(\"depth\", dat_surf.depth.data, dtype=pl.Float32),\n",
    "    pl.Series(\"bbp\", dat_surf.bbp700.data, dtype=pl.Float32),\n",
    "    pl.Series(\"fluo\", flr_base.data, dtype=pl.Float32),\n",
    "]\n",
    "\n",
    "df = pl.DataFrame(data)\n",
    "\n",
    "#define a function to detect if the profile is at day or night\n",
    "def classify_day_night(dt):\n",
    "    hour = dt.hour\n",
    "    return \"day\" if 6 <= hour < 21 else \"night\"\n",
    "\n",
    "# Add the new column\n",
    "df = df.with_columns(\n",
    "    pl.col(\"datetime\").map_elements(lambda dt: classify_day_night(dt)).alias(\"day_night\")\n",
    ")\n",
    "\n",
    "# Extract date part from datetime\n",
    "df = df.with_columns(pl.col(\"datetime\").dt.date().alias(\"date\"))\n",
    "df = df.with_columns(pl.col(\"depth\").round(0).alias(\"depth_rounded\"))\n",
    "night_profiles = df.filter(\n",
    "    (pl.col(\"day_night\") == \"night\") & (pl.col(\"depth\") <= 40)\n",
    ")\n",
    "\n",
    "# Compute the night fluo/bbp ratio\n",
    "night_profiles = night_profiles.with_columns(\n",
    "    (pl.col(\"fluo\") / pl.col(\"bbp\")).alias(\"fluo_bbp_ratio\")\n",
    ")\n",
    "\n",
    "night_ratios_by_depth = night_profiles.group_by([\"date\"]).agg(\n",
    "    pl.col(\"fluo_bbp_ratio\").drop_nans().mean().alias(\"night_depth_ratio\")\n",
    ")\n",
    "\n",
    "#compute the ratio for day profiles\n",
    "day_profiles = df.filter(\n",
    "    (pl.col(\"day_night\") == \"day\") & (pl.col(\"depth\") <= 40)\n",
    ").with_columns(\n",
    "    (pl.col(\"fluo\") / pl.col(\"bbp\")).alias(\"fluo_bbp_ratio\")\n",
    ")\n",
    "\n",
    "#detect the quenched layer\n",
    "quenched_layer = day_profiles.group_by(\"prof\"\n",
    ").agg(pl.col(\"fluo\").drop_nans().max().alias(\"max_fluo_value\")\n",
    ")\n",
    "\n",
    "max_depth_per_profile = (\n",
    "    df.join(quenched_layer, on=[\"prof\"])\n",
    "    .filter(pl.col(\"fluo\") == pl.col(\"max_fluo_value\"))\n",
    "    .select([\"prof\", \"max_fluo_value\", \"depth\"])\n",
    "    .rename({\"depth\": \"max_fluo_depth\"})\n",
    ").group_by(pl.col(\"prof\")\n",
    ").agg(pl.col(\"max_fluo_depth\").drop_nans().mean())\n",
    "\n",
    "# Join day profiles with corresponding night ratios by date and depth\n",
    "day_profiles = day_profiles.join(\n",
    "    night_ratios_by_depth, on=[\"date\"], how=\"left\"\n",
    ").join(max_depth_per_profile, on = [\"prof\"], how = \"left\")\n",
    "# Define a significant drop (e.g., 20% drop from night ratio)\n",
    "day_profiles = day_profiles.with_columns(\n",
    "    ((pl.col(\"fluo_bbp_ratio\") < pl.col(\"night_depth_ratio\") * 0.95) & (pl.col(\"depth\") < pl.col(\"max_fluo_depth\")))\n",
    "    .alias(\"quenching_detected\")\n",
    ")\n",
    "day_profiles = day_profiles.with_columns(\n",
    "    pl.when(pl.col(\"quenching_detected\"))\n",
    "    .then(pl.col(\"bbp\") * pl.col(\"night_depth_ratio\"))\n",
    "    .otherwise(pl.col(\"fluo\"))\n",
    "    .alias(\"fluo_unquenched\")\n",
    ")\n",
    "df = df.join(day_profiles.select([\"datetime\", \"depth\", \"fluo_unquenched\"]), \n",
    "              on=[\"datetime\", \"depth\"], how=\"left\")\n",
    "\n",
    "# Fill night profiles or uncorrected points with original fluo values\n",
    "df = df.with_columns(\n",
    "    pl.when(pl.col(\"fluo_unquenched\").is_null())\n",
    "    .then(pl.col(\"fluo\"))\n",
    "    .otherwise(pl.col(\"fluo_unquenched\"))\n",
    "    .alias(\"fluo_unquenched\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_surf['fluo_corrected'] = (\"time\", df[\"fluo_unquenched\"])\n",
    "dat_surf['fluo'] = (\"time\", df[\"fluo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flr_qc, quench_layer = gt.optics.quenching_correction(\n",
    "#     flr_base, dat_surf.bbp700, dat_surf.profile_index, dat_surf.depth, dat_surf.time.data.astype('datetime64[ms]'), dat_surf.latitude, dat_surf.longitude,\n",
    "#     sunrise_sunset_offset=1, night_day_group = True, max_photic_depth = 50)\n",
    "\n",
    "# fig, ax = plt.subplots(2, 1, figsize=[9, 6], sharex=True, dpi=90)\n",
    "\n",
    "# gt.plot(dat_surf['profile_index'], dat_surf['depth'], flr_qc, cmap=cmo.delta, ax=ax[0], robust=True)\n",
    "# gt.plot(dat_surf['profile_index'], dat_surf['depth'], quench_layer, cmap=plt.cm.RdBu_r, ax=ax[1], vmin=-.5, vmax=2)\n",
    "\n",
    "# [a.set_xlabel('') for a in ax]\n",
    "# [a.set_ylim(100, 0) for a in ax]\n",
    "\n",
    "# ax[0].set_title('Quenching Corrected Fluorescence')\n",
    "# ax[1].set_title('Quenching Layer')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=[9, 6], sharex=True, dpi=90)\n",
    "\n",
    "gt.plot(dat_surf['profile_index'], dat_surf['depth'], dat_surf['fluo'], cmap=cmo.delta, ax=ax[0], robust=True)\n",
    "gt.plot(dat_surf['profile_index'], dat_surf['depth'], dat_surf['fluo_corrected'], cmap=cmo.delta, ax=ax[1], robust=True)\n",
    "gt.plot(dat_surf['profile_index'], dat_surf['depth'], dat_surf['bbp700'], cmap=cmo.delta, ax=ax[2])\n",
    "\n",
    "[a.set_xlabel('') for a in ax]\n",
    "[a.set_ylim(100, 0) for a in ax]\n",
    "#[a.set_xlim(400, 450) for a in ax]\n",
    "\n",
    "ax[0].set_title(\"Raw Fluorescence\")\n",
    "ax[1].set_title('Quenching Corrected Fluorescence')\n",
    "ax[2].set_title('bbp')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_surf['fluo_dark'] = (\"time\", flr_dark.data)\n",
    "#dat_surf['fluo_unquenched'] = (\"time\", flr_qc.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (dat_surf.profile_index == 440).compute()\n",
    "prof_i = dat_surf.where(mask, drop = True)\n",
    "\n",
    "flr_raw = prof_i['fluo']\n",
    "flr_unquenched = prof_i['fluo_corrected']\n",
    "depths = prof_i['depth'].compute()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(flr_raw, depths, label='Fluorescence Raw', color='blue')\n",
    "plt.plot(flr_unquenched, depths, label='Fluorescence Unquenched', color='orange', linestyle='--')\n",
    "\n",
    "plt.gca().invert_yaxis()  # Depth increases downwards\n",
    "plt.xlabel(\"Fluorescence\")\n",
    "plt.ylabel(\"Depth (m)\")\n",
    "plt.legend()\n",
    "plt.title(\"Profile: Fluorescence at Night\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (dat_surf.profile_index == 372).compute()\n",
    "prof_i = dat_surf.where(mask, drop = True)\n",
    "\n",
    "flr_raw = prof_i['fluo']\n",
    "flr_unquenched = prof_i['fluo_corrected']\n",
    "depths = prof_i['depth'].compute()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(flr_raw, depths, label='Fluorescence Raw', color='blue')\n",
    "plt.plot(flr_unquenched, depths, label='Fluorescence Unquenched', color='orange', linestyle='--')\n",
    "\n",
    "plt.gca().invert_yaxis()  # Depth increases downwards\n",
    "plt.xlabel(\"Fluorescence\")\n",
    "plt.ylabel(\"Depth (m)\")\n",
    "plt.legend()\n",
    "plt.title(\"Profile: Fluorescence at day\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Satellite PAR matchup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory where your NetCDF files are stored (daily files)\n",
    "ncdf_dir = \"C:/Users/flapet/OneDrive - NOC/Documents/IDAPro/lib/db_building/data/satellite/par_mapped/\"\n",
    "\n",
    "# List all NetCDF files for the year (one file per day)\n",
    "ncdf_files = sorted([f for f in os.listdir(ncdf_dir) if f.endswith('.nc')])\n",
    "\n",
    "# Load the PAR data for each file and store it\n",
    "daily_par_data = {}\n",
    "for file in ncdf_files:\n",
    "    # Open the dataset for the specific day\n",
    "    ds = xr.open_dataset(os.path.join(ncdf_dir, file))\n",
    "    \n",
    "    # Extract PAR data (assuming it's named 'par', adjust if necessary)\n",
    "    # Add a date key to use as the dictionary key for each day\n",
    "    date = file.split('.')[1] # Extract the date from the filename (adjust based on filename format)\n",
    "    \n",
    "    # Store the data in a dictionary, with the date as the key\n",
    "    daily_par_data[date] = ds[\"par\"]  # Replace 'par' with the actual variable name in your files\n",
    "\n",
    "# Example: inspect one of the datasets\n",
    "print(daily_par_data[\"20240101\"])  # Inspect the PAR data for January 1st, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "import datetime\n",
    "\n",
    "# Function to find the nearest PAR value for each observation\n",
    "def get_nearest_par(lat, lon, date_str, daily_par_data):\n",
    "    # Check if the date is in the daily PAR data\n",
    "    if date_str in daily_par_data and daily_par_data[date_str].size > 0:\n",
    "        # Get PAR data for that date and find the nearest point\n",
    "        par_data = daily_par_data[date_str]\n",
    "        nearest_par = par_data.sel(lon=lon, lat=lat, method='nearest').values.flatten()[0]\n",
    "    else:\n",
    "        nearest_par = 0  # Default if no data for that date\n",
    "    return nearest_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = dat_surf['longitude'].data\n",
    "lat = dat_surf['latitude'].data\n",
    "time =  [datetime.datetime.utcfromtimestamp(t.astype('datetime64[s]').astype(int)).strftime(\"%Y%m%d\") for t in dat_surf['time'].data]\n",
    "prof_index = dat_surf['profile_index'].data\n",
    "\n",
    "df = pd.DataFrame({'lon' : lon, 'lat' : lat, 'time' : time, 'profile_index' : prof_index})\n",
    "profile_means = (\n",
    "    df.groupby(\"profile_index\")[[\"lat\", \"lon\"]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_series = df.groupby(['profile_index','time']).size().reset_index().rename(columns={0:'count'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_match = pd.merge(profile_means, dates_series, how='left',on=['profile_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_match.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Step 3: Match PAR for each profile (with progress bar)\n",
    "par_per_profile = []\n",
    "for _, row in tqdm(df_to_match.iterrows(), total=len(df_to_match)):\n",
    "    nearest_par = get_nearest_par(row[\"lat\"], row[\"lon\"], row[\"time\"], daily_par_data)\n",
    "    par_per_profile.append(nearest_par)\n",
    "\n",
    "# Add PAR to the profile_means DataFrame\n",
    "df_to_match[\"PAR_matched\"] = par_per_profile\n",
    "\n",
    "# Step 4: Merge PAR values back into the original DataFrame\n",
    "df = df.merge(df_to_match[[\"profile_index\", \"PAR_matched\"]], on=\"profile_index\")\n",
    "\n",
    "df.drop_duplicates(subset=['profile_index'], inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xr = df.to_xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure DataFrame has the profile_index and PAR columns\n",
    "df = df[[\"profile_index\", \"PAR_matched\"]]\n",
    "\n",
    "# Step 1: Merge DataFrame with xarray dataset's profile_index as a DataFrame\n",
    "dat_surf_df = dat_surf.to_dataframe().reset_index()\n",
    "\n",
    "# Step 2: Perform a left join on profile_index\n",
    "merged_df = dat_surf_df.merge(df, on=\"profile_index\", how=\"left\")\n",
    "\n",
    "# Step 3: Convert the merged DataFrame back into xarray\n",
    "dat_surf = merged_df.set_index([\"time\"]).to_xarray()\n",
    "\n",
    "# Step 4: Ensure the new PAR column is added properly\n",
    "dat_surf['PAR'] = dat_surf['PAR_matched']\n",
    "dat_surf = dat_surf.drop_vars('PAR_matched')  # Optional: drop the extra 'par' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Keep only rows with depth between 0 and 201 meters\n",
    "filtered_df = merged_df[(merged_df['depth'] >= 0) & (merged_df['depth'] <= 201)].copy()\n",
    "\n",
    "# Step 2: Create 1-meter bins (0 to 201 inclusive)\n",
    "filtered_df['depth_bin'] = np.floor(filtered_df['depth']).astype(int)\n",
    "\n",
    "# Step 3: Group by profile and depth bin, then average values\n",
    "averaged_df = (\n",
    "    filtered_df\n",
    "    .groupby(['profile_index', 'depth_bin'])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 4: Sort for cleaner output\n",
    "averaged_df = averaged_df.sort_values(['profile_index', 'depth_bin']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_df['BBP470'] = averaged_df['bbp700']/(470/400) \n",
    "averaged_df['carbon'] = 12128 * averaged_df['BBP470'] + 0.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_df['time'] = pd.to_datetime(averaged_df['time'])\n",
    "\n",
    "# Extract year, month, and day into new columns\n",
    "averaged_df['year'] = averaged_df['time'].dt.year\n",
    "averaged_df['month'] = averaged_df['time'].dt.month\n",
    "averaged_df['day'] = averaged_df['time'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the complete range of depth bins\n",
    "depth_bins_full = list(range(201))\n",
    "\n",
    "# Get the unique profile indices to ensure we process each profile separately\n",
    "profile_indices = averaged_df['profile_index'].unique()\n",
    "\n",
    "# Process each profile individually\n",
    "new_rows = []\n",
    "for profile in profile_indices:\n",
    "    profile_df = averaged_df[averaged_df['profile_index'] == profile]\n",
    "    \n",
    "    # Only proceed if the profile has more than 180 rows\n",
    "    if len(profile_df) > 180:\n",
    "        existing_bins = set(profile_df['depth_bin'])\n",
    "        missing_bins = [bin for bin in depth_bins_full if bin not in existing_bins]\n",
    "        \n",
    "        if missing_bins:\n",
    "            # Take the first row of the existing profile to replicate\n",
    "            first_row = profile_df.iloc[0].copy()\n",
    "            \n",
    "            for missing_bin in missing_bins:\n",
    "                new_row = first_row.copy()\n",
    "                new_row['depth_bin'] = missing_bin\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "# Add the new rows to the original DataFrame if any rows were created\n",
    "if new_rows:\n",
    "    averaged_df = pd.concat([averaged_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "# Sort the DataFrame to keep depth_bin order neat\n",
    "averaged_df = averaged_df.sort_values(by=['profile_index', 'depth_bin']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_df[averaged_df['profile_index'] == 282].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time constants\n",
    "day_start = 6  # Dawn (6 AM)\n",
    "day_end = 20  # Dusk (6 PM)\n",
    "t_noon = 13  # Noon time\n",
    "day_length = day_end - day_start\n",
    "\n",
    "# Extract hour of day as a decimal (e.g., 8:30 → 8.5)\n",
    "averaged_df[\"hour_of_day\"] = averaged_df[\"time\"].dt.hour + averaged_df[\"time\"].dt.minute / 60\n",
    "\n",
    "# Define the weighted PAR function\n",
    "def compute_weighted_par(hour, par_noon):\n",
    "    \"\"\"Compute weighted PAR based on time of day.\"\"\"\n",
    "    if hour  < day_start or hour > day_end:\n",
    "        weight = 0\n",
    "    else:\n",
    "        weight = max(0, np.cos((np.pi * (hour - t_noon)) / (day_length / 2)))\n",
    "    return par_noon * weight\n",
    "\n",
    "# Apply the function row-wise to compute weighted PAR values\n",
    "averaged_df[\"PAR_weighted\"] = averaged_df.apply(\n",
    "    lambda row: compute_weighted_par(row[\"hour_of_day\"], row[\"PAR_matched\"]), axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(array, window_size=5):\n",
    "    kernel = np.ones(window_size) / window_size\n",
    "    smoothed_array = np.convolve(array, kernel, mode='same')  # 'same' ensures the output matches input size\n",
    "    return smoothed_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cbpm_argo import cbpm_argo\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "dfs = []\n",
    "depth_grid = np.arange(0,200)\n",
    "\n",
    "# Iterate through each unique profile index\n",
    "for i in averaged_df['profile_index'].unique():\n",
    "    # Filter for rows corresponding to the current 'JULD'\n",
    "    temp_df = averaged_df[averaged_df['profile_index'] == i].iloc[0:200,].copy()  # Use `.copy()` to avoid warnings\n",
    "\n",
    "    # Extract the pressure and chlorophyll values for interpolation\n",
    "    pres_values = temp_df['depth_bin'].to_numpy()\n",
    "    chl_values = temp_df['fluo_corrected'].to_numpy()\n",
    "    carbon_values = temp_df['carbon'].to_numpy()\n",
    "\n",
    "    # Apply the running mean smoothing\n",
    "    chl_smoothed = running_mean(chl_values, window_size=5)\n",
    "    carbon_smoothed = running_mean(carbon_values, window_size=5)\n",
    "\n",
    "    # Check for valid data before interpolation (avoid NaN values)\n",
    "    mask = ~np.isnan(pres_values) & ~np.isnan(chl_values)\n",
    "    pres_values = pres_values[mask]\n",
    "    chl_values = chl_values[mask]\n",
    "    carbon_values = carbon_values[mask]\n",
    "\n",
    "    # Interpolate the CHLA_ADJUSTED onto the depth grid (0 to 199)\n",
    "    if len(pres_values) > 1:  # Ensure there's enough data to interpolate\n",
    "        interpolator = interp1d(pres_values, chl_values, bounds_error=False, fill_value=np.nan)\n",
    "        interpolated_chl = interpolator(depth_grid)\n",
    "        \n",
    "        interpolator = interp1d(pres_values, carbon_values, bounds_error=False, fill_value=np.nan)\n",
    "        interpolated_carbon = interpolator(depth_grid)\n",
    "        \n",
    "    else:\n",
    "        # If only one point or no valid data, fill with NaN\n",
    "        print(temp_df['profile_index'].unique())\n",
    "        interpolated_chl = np.full(depth_grid.shape, np.nan)\n",
    "        interpolated_carbon = np.full(depth_grid.shape, np.nan)\n",
    "\n",
    "    # Now we can extract other values and apply the cbpm_argo function\n",
    "    chl_z = interpolated_chl\n",
    "    Cphyto_z = interpolated_carbon\n",
    "    irr = temp_df['PAR_weighted'].mean()  # Mean irradiance value\n",
    "    year = int(temp_df['year'].mean())\n",
    "    month = int(temp_df['month'].mean())\n",
    "    day = int(temp_df['day'].mean())\n",
    "    lat = temp_df['latitude'].mean()\n",
    "\n",
    "    # Call the cbpm_argo function with interpolated data\n",
    "    [pp_z, mu_z, par_z, prcnt_z, nutTempFunc_z, IgFunc_z, mzeu] = cbpm_argo(chl_z, Cphyto_z, irr, year, month, day, lat)\n",
    "\n",
    "    size_max = len(temp_df)\n",
    "\n",
    "    # Use .loc to explicitly assign new columns (expand results back into DataFrame)\n",
    "    temp_df.loc[:, 'pp'] = pp_z[0:size_max]\n",
    "    temp_df.loc[:, 'mu'] = mu_z[0:size_max]\n",
    "    temp_df.loc[:, 'prcnt'] = prcnt_z[0:size_max]\n",
    "    temp_df.loc[:, 'nutTempFunc'] = nutTempFunc_z[0:size_max]\n",
    "    temp_df.loc[:, 'IgFunc'] = IgFunc_z[0:size_max]\n",
    "    temp_df.loc[:, 'zeu'] = np.full(size_max, mzeu)\n",
    "\n",
    "    # Append modified DataFrame to the list\n",
    "    dfs.append(temp_df)\n",
    "\n",
    "\n",
    "# Combine all DataFrames\n",
    "final_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_final = final_df.set_index([\"time\"]).to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_df[final_df['profile_index'] == 282])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.iloc[0:201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=[9, 6], sharex=True, dpi=90)\n",
    "\n",
    "gt.plot(dat_final['profile_index'], dat_final['depth'], dat_final['fluo_corrected'], cmap=cmo.delta, ax=ax[0], robust=True)\n",
    "gt.plot(dat_final['profile_index'], dat_final['depth'], dat_final['bbp700'], cmap=cmo.matter, ax=ax[1], robust=True)\n",
    "gt.plot(dat_final['profile_index'], dat_final['depth'], dat_final['pp'], cmap=plt.cm.RdBu_r, ax=ax[2])\n",
    "\n",
    "[a.set_xlabel('') for a in ax]\n",
    "[a.set_ylim(100, 0) for a in ax]\n",
    "#[a.set_xlim(265, 275) for a in ax]\n",
    "\n",
    "ax[0].set_title(\"Fluorescence\")\n",
    "ax[1].set_title('Carbon phyto')\n",
    "ax[2].set_title('NPP (CbPM)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat_final = dat_final.drop_vars(['PAR_matched_x', 'PAR_matched_y'])\n",
    "display(dat_final)\n",
    "dat_final.to_netcdf('C:/Users/flapet/OneDrive - NOC/Documents/IDAPro/lib/db_building/data/glider/npp_ncdf/cabot_npp.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
